{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.716 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "CPU times: user 2.15 s, sys: 232 ms, total: 2.39 s\nWall time: 22.7 s\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "%%time\n",
    "from tool import *\n",
    "import pandas as pd\n",
    "\n",
    "boards = ['Gossiping', 'C_Chat', 'WomenTalk', 'Baseball', 'HatePolitics', 'NBA']\n",
    "\n",
    "df = pd.DataFrame(get_data(connect_db(), {'board': {'$in': boards}, }, projection={\n",
    "    'raw_title':1, 'raw_text': 1, '_id': 0, 'board': 1\n",
    "}))\n",
    "\n",
    "df['raw_corpus'] = df['raw_title']+ ' ' + df['raw_text']\n",
    "sp = SentenceProcessor()\n",
    "# print(df.head())\n",
    "corpus = [sp.cut_and_remove(title) for title in df['raw_corpus']]\n",
    "answer = df['board']"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bag of Words 詞袋模型"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# BOW\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def get_bow(corpus):\n",
    "    count_vec = CountVectorizer(stop_words='english', max_features=500)\n",
    "    # fit corpus\n",
    "    x_bow = count_vec.fit_transform([' '.join(sentence) for sentence in corpus])\n",
    "    return np.array(x_bow.toarray()), count_vec\n",
    "\n",
    "x_bow, count_vec = get_bow(corpus)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TF-IDF \n",
    "\n",
    "* sklearn TfidfVectorizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# tfidf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def get_tfidf(corpus):\n",
    "    tfidf = TfidfVectorizer(stop_words='english', max_features=500)\n",
    "    x_tfidf = tfidf.fit_transform([' '.join(sentence) for sentence in corpus])\n",
    "    x_tfidf = np.array(x_tfidf.toarray())\n",
    "    return x_tfidf, tfidf\n",
    "\n",
    "x_tfidf, tfidf = get_tfidf(corpus)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "19751\n",
      "CPU times: user 20.3 s, sys: 256 ms, total: 20.5 s\nWall time: 9.43 s\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "/home/user/miniconda3/envs/spider/lib/python3.7/site-packages/ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "%%time\n",
    "# word2vec\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def build_word2vec_model(size=100, window=5, min_count=5):\n",
    "    model = Word2Vec(corpus, size=size, window=window, workers=4, min_count=min_count)\n",
    "    model.save('models/w2v_size_{}_window_{}_min_count_{}.model'.format(size, window, min_count))\n",
    "    return model\n",
    "\n",
    "def get_avg_vector(content, word2id, w2v_model, size=100):\n",
    "    ans = np.array(np.zeros(size, ), dtype=\"float32\")\n",
    "    cnt = 0\n",
    "    for word in content:\n",
    "        if word in word2id.keys():\n",
    "            cnt += 1\n",
    "            ans = np.add(ans, w2v_model[word])\n",
    "    return np.divide(ans, cnt)\n",
    "\n",
    "def get_w2v(corpus, word2id, model, size):\n",
    "    return np.array([get_avg_vector(sentence, word2id, model, size) for sentence in corpus])\n",
    "\n",
    "w2v_model = build_word2vec_model(size=300, window=5, min_count=5)\n",
    "word2id ={v :k+1 for k, v in enumerate(w2v_model.wv.vocab.keys())}\n",
    "print(len(word2id))\n",
    "x_w2v = get_w2v(corpus, word2id, w2v_model, size=300) "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# 切割資料\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def data_split(X, y, test_size=0.2, random_state=40):\n",
    "    return train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "X_train_bow, X_test_bow, y_train_bow, y_test_bow = data_split(x_bow, answer)\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = data_split(x_tfidf, answer)\n",
    "X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v = data_split(x_w2v, answer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "/home/user/miniconda3/envs/spider/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n  warnings.warn(msg, category=DeprecationWarning)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# save model\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "def save_model(model, file):\n",
    "    joblib.dump(model, 'models/{}'.format(file))\n",
    "    \n",
    "def load_model(file):\n",
    "    return joblib.load('models/{}'.format(file))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "0.7951176983435048\n              precision    recall  f1-score   support\n\n    Baseball       0.92      0.80      0.85       177\n      C_Chat       0.73      0.65      0.69       193\n   Gossiping       0.85      0.71      0.78       182\nHatePolitics       0.81      0.83      0.82       204\n         NBA       0.92      0.93      0.93       192\n   WomenTalk       0.63      0.84      0.72       199\n\n    accuracy                           0.80      1147\n   macro avg       0.81      0.79      0.80      1147\nweighted avg       0.81      0.80      0.80      1147\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# MultinomialNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "def MNB(X_train, y_train, X_test, y_test):\n",
    "    mnb = MultinomialNB()\n",
    "    mnb.fit(X_train, y_train)\n",
    "    print(mnb.score(X_test, y_test))\n",
    "    y_predict = mnb.predict(X_test)\n",
    "    print(classification_report(y_test, y_predict))\n",
    "    return mnb\n",
    "\n",
    "mnb = MNB(X_train_bow, y_train_bow, X_test_bow, y_test_bow)\n",
    "save_model(mnb, 'MultinomialNB.model')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "0.8605056669572798\n",
      "              precision    recall  f1-score   support\n\n    Baseball       0.96      0.90      0.93       177\n      C_Chat       0.80      0.79      0.79       193\n   Gossiping       0.90      0.90      0.90       182\nHatePolitics       0.82      0.84      0.83       204\n         NBA       0.95      0.96      0.96       192\n   WomenTalk       0.76      0.79      0.77       199\n\n    accuracy                           0.86      1147\n   macro avg       0.86      0.86      0.86      1147\nweighted avg       0.86      0.86      0.86      1147\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# SVM\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "def get_svc_model(X_train, y_train, X_test, y_test):\n",
    "    svc = LinearSVC()\n",
    "    svc.fit(X_train, y_train)\n",
    "    print(svc.score(X_test, y_test))\n",
    "    y_predict = svc.predict(X_test)\n",
    "    print(classification_report(y_test, y_predict))\n",
    "    return svc\n",
    "\n",
    "svc = get_svc_model(X_train_tfidf, y_train_tfidf, X_test_tfidf, y_test_tfidf)\n",
    "save_model(mnb, 'linearSVC.model')\n",
    "# svc.predict(get_w2v([document], word2id, w2v_model))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "/home/user/miniconda3/envs/spider/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "0.6521360069747166\n              precision    recall  f1-score   support\n\n    Baseball       0.75      0.79      0.77       177\n      C_Chat       0.53      0.60      0.56       193\n   Gossiping       0.44      0.45      0.45       182\nHatePolitics       0.81      0.71      0.76       204\n         NBA       0.86      0.84      0.85       192\n   WomenTalk       0.55      0.53      0.54       199\n\n    accuracy                           0.65      1147\n   macro avg       0.66      0.65      0.65      1147\nweighted avg       0.66      0.65      0.65      1147\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def get_rf_model(X_train, y_train, X_test, y_test):\n",
    "    rf = RandomForestClassifier()\n",
    "    rf.fit(X_train, y_train)\n",
    "    print(rf.score(X_test, y_test))\n",
    "    y_predict = rf.predict(X_test)\n",
    "    print(classification_report(y_test, y_predict))\n",
    "    return rf\n",
    "\n",
    "rf = get_rf_model(X_train_w2v, y_train_w2v, X_test_w2v, y_test_w2v)\n",
    "save_model(rf, 'randomForest.model')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}